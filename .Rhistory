tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
library(dplyr)
library(quanteda) # includes functions to implement Lexicoder
library(quanteda.textmodels) # for estimating similarity and complexity measures
install.packages(quanteda.textmodels)
install.pacakges(quanteda.textplots)
install.packages(quanteda.textmodels)
install.packages(quanteda.textplots)
install.packages("quanteda.textmodels")
install.packages("quanteda.textplots")
library(dplyr)
library(quanteda) # includes functions to implement Lexicoder
library(quanteda.textmodels) # for estimating similarity and complexity measures
library(quanteda.textplots)
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
tweets <- tweets %>%
sample_n(20000)
#make corpus object, specifying tweet as text field
tweets_corpus <- corpus(tweets, text_field = "text")
#add in username document-level information
docvars(tweets_corpus, "newspaper") <- tweets$user_username
dfm_tweets <- dfm(tokens(tweets_corpus,
remove_punct = TRUE)) %>%
dfm_select(pattern = stopwords("english"),
selection = "remove",
valuetype = "fixed")
## number of tweets per newspaper
table(docvars(dfm_tweets, "newspaper"))
# compress the document-feature matrix at the newspaper level
dfm_newstweets <- dfm_group(dfm_tweets, groups = newspaper)
# remove words not used by two or more newspapers
dfm_newstweets <- dfm_trim(dfm_newstweets,
min_docfreq = 2, docfreq_type = "count")
## size of the document-feature matrix
dim(dfm_newstweets)
#### estimate the Wordfish model ####
set.seed(123L) #what does this do?)
dfm_newstweets_results <- textmodel_wordfish(dfm_newstweets,
sparse = TRUE)
```
summary(dfm_newstweets_results)
textplot_scale1d(dfm_newstweets_results)
textplot_scale1d(dfm_newstweets_results, margin = "features")
features <- dfm_newstweets_results[["features"]]
betas <- dfm_newstweets_results[["beta"]]
feat_betas <- as.data.frame(cbind(features, betas))
feat_betas$betas <- as.numeric(feat_betas$betas)
feat_betas %>%
arrange(desc(betas)) %>%
top_n(20) %>%
kbl() %>%
kable_styling(bootstrap_options = "striped")
library(kableExtra)
features <- dfm_newstweets_results[["features"]]
betas <- dfm_newstweets_results[["beta"]]
feat_betas <- as.data.frame(cbind(features, betas))
feat_betas$betas <- as.numeric(feat_betas$betas)
feat_betas %>%
arrange(desc(betas)) %>%
top_n(20) %>%
kbl() %>%
kable_styling(bootstrap_options = "striped")
kaneko_dfm  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/wordscaling/study1_kaneko.rds?raw=true")))
table(docvars(kaneko_dfm, "Newspaper"))
## prepare the newspaper-level document-feature matrix
# compress the document-feature matrix at the newspaper level
kaneko_dfm_study1 <- dfm_group(kaneko_dfm, groups = Newspaper)
# remove words not used by two or more newspapers
kaneko_dfm_study1 <- dfm_trim(kaneko_dfm_study1, min_docfreq = 2, docfreq_type = "count")
## size of the document-feature matrix
dim(kaneko_dfm_study1)
#### estimate the Wordfish model ####
set.seed(456L)
dfm_kaneko_results <- textmodel_wordfish(kaneko_dfm_study1,
sparse = TRUE)
summary(dfm_kaneko_results)
textplot_scale1d(dfm_newstweets_results)
textplot_scale1d(dfm_kaneko_results)
textplot_scale1d(dfm_kaneko_results, margin = "features")
Head(kaneko_dfm_study1)
head(kaneko_dfm_study1)
sysfonts
head(kaneko_dfm_study1)# here we see that there are Kanji.
features_k <- dfm_kaneko_results[["features"]]
betas_k<- dfm_kaneko_results[["beta"]]
feat_betas_k <- as.data.frame(cbind(features_k, betas))
feat_betas_k$betas <- as.numeric(feat_betas_k$betas)
feat_betas_k %>%
arrange(desc(betas)) %>%
top_n(20) %>%
kbl() %>%
kable_styling(bootstrap_options = "striped")
features_k
summary(dfm_kaneko_results)%>% # we look at the summary scores for the documents.
arrange(desc(theta))
summary(dfm_kaneko_results)%>% # we look at the summary scores for the documents.
summary(dfm_kaneko_results)%>% # we look at the summary scores for the documents.
summary(dfm_kaneko_results)
summary(dfm_kaneko_results)
class(summary(dfm_kaneko_results))
as.tibble(summary(dfm_kaneko_results))
summary_kaneko<-summary(dfm_kaneko_results)%>% #we create the summary
as.data.frame #
summary_kaneko<-as.data.frame(summary(dfm_kaneko_results)) # we create the summary, make it a df
set.seed(456L)
dfm_kaneko_results <- textmodel_wordfish(kaneko_dfm_study1,
sparse = TRUE) # we apply wordfish to the model
summary_kaneko<-as.data.frame(summary(dfm_kaneko_results)) # we create the summary, make it a df
summary_kaneko<-summary(dfm_kaneko_results) # we create the summary, make it a df
summary_kaneko
?sttstcs_
??sttstcs_
class(summary_kaneko)
typeof(summary_kaneko)
as.data.frame(summary_kaneko)
as.data.frame(summary_kaneko$estimated.document.positions)
summary_kaneko<-summary(dfm_kaneko_results) # we create the summary, make it a df
as.data.frame(summary_kaneko$estimated.document.positions)%>% # we look at the summary scores for the documents.
arrange(desc(theta))
summary_groups<-as.data.frame(summary_kaneko$estimated.document.positions)%>% # we look at the summary scores for the documents.
arrange(desc(theta))
library(dplyr)
summary_kaneko<-summary(dfm_kaneko_results) # we create the summary, make it a df
summary_groups<-as.data.frame(summary_kaneko$estimated.document.positions)%>% # we look at the summary scores for the documents.
arrange(desc(theta))
summary_groups<-as.data.frame(summary_kaneko$estimated.document.positions)
summary_kaneko<-summary(dfm_kaneko_results) # we create the summary, make it a df
summary_groups<-as.data.frame(dfm_kaneko_results$estimated.document.positions)
summary_groups
